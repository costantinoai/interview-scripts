# interview-scripts

A few polished scripts for postdoc interviews. The code is in Python and uploaded in Jupyter Notebooks for easy visualization. You can download the script or open them in your browser. More info:

## AlexNet_embeddings
How can we know if perceptual (dis)similarity can reasonably explain a pattern of results? In other words: what categories of images are perceptually distinct from each other, and what can this tell us about the brain? I wrote this script to explore the idea that the pattern of results we saw in our fMRI decoding analysis could be replicated by a computational model mimicking the early layers of the ventral visual stream.

This script is part of an fMRI pipeline. In this study, we showed visual stimuli belonging to 2 categories (faces, vehicles) and 4 sub-categories (cars, bikes, male and female faces) to 24 subjects. We collected, preprocessed and analyzed the fMRI data, and performed a Multi Voxel Pattern Analysis on the beta images. The results from this analysis showed above-chance accuracy in some ROIs for all the possible comparisons, except for perceptually similar categories (e.g., female vs male faces). But how do we know what categories are perceptually similar? One solution that I implement here is to feed our images to a computational model performing basic low-level perceptual tasks and run a similar classification analysis on the network's activation. Here we chose the first convolutional layer of AlexNet. If we see above-chance classification accuracy between two categories of stimuli, we can assume that these categories are perceptually different for the network. Answering this question can help us understand the nature (categorical vs. perceptual) of the information we found in our fMRI data.

This is what the script does in a nutshell: we import the images, load the network and plot the weights of the first convolutional layer. We pass all the images through the model, extracting the activation from the first convolutional layer for each image. We apply different levels of noise to the dataset, reduce the dimensionality and perform a classification analysis. Finally, we plot the results.

## fMRI_MVPA_from_SPM_betas
How can we know what information is coded in a specific region of interest? Does the foveal cortex process information about peripheral stimuli? And what is the nature ( categorical vs perceptual) of the information? 

I wrote this script to perform an ROI MVPA analysis on fMRI data to answer these questions. This script uses precomputed retinotopic and functional Regions of Interest (ROIs) to mask beta images (see the section above for some more info about the study) generated by SPM. We perform a cross-validated (GroupKFold) classification analysis on each ROI and for all the comparisons (e.g., bike vs cars, etc.) independently. We can decide if we want to perform feature selection on the dataset and if we want to find the optimal classifier parameters with a cross-validated grid search. To import the beta images, we interact with the SPM.mat file from Python to map the betas. We finally plot the results.

## visualize_retinotopy
This script showcases three main things: 
  1. an implementation of a surface-based retinotopic mapping method developed by Noah Benson. I used this method to generate retinotopic ROIs based on subjects' anatomy that I then used to mask fMRI data for subsequent analysis. 
  2. an interesting function to pass nuisance regressors from fMRIprep to nilearn for denoising 
  3. extraction and plotting of time series from surface data (with and without denoising strategies)

## Bonus: computational model
[Here](https://github.com/costantinoai/deepnet-scripts/blob/main/same_different_cornet_Z.ipynb) you can find a less polished notebook (hence why I didn't include it here) where we define a new computational model based on the [CORnet Paper](https://www.biorxiv.org/content/10.1101/408385v1). In the script we generate peripheral pairs of stimuli (same or different) in two orientations (normal: down_left, up_right; inverted: up_left, down_right) for 4 categories (cars, bikes, male and female faces). Then we define our CORnet model as described in the original paper, we import pretrained (on ImageNet) weights and biases and, since we want to do a same-different classification and so we have only two possible outputs (as opposed to the 1000 labels in ImageNet) we modify the last lyer of the network. We freeze all the layers except the last one, and we retrain the network on our new stimuli. After a short training, we (slightly) improved the accuracy of the model from chance. A open issue here is overfitting: we know the model is not performing good because of the small amount of data (n = 240) we have. Future goals: apply augmentations techniques on the stimuli, find the optimal learning rate and batch size, feature reduction (we have more features than observations!).
