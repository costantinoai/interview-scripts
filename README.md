# interview-scripts
A few polished scripts for postdoc interviews. All the scripts are in Python and uploaded in Jupyter Notebooks for easy visualization. You can download the script or just open them in your browser. More info:

## AlexNet_embeddings
How can we know if a pattern of results can be reasonably explained by perceptual (dis)similarity? In other words: what categories of images are perceptually distinct from each other, and what can this tell us about the brain? I wrote this script to explore the idea that the pattern of results we saw in our fMRI decoding analysis could be replicated by a computational model mimicking the early layers of the ventral visual stream. 

This script is part of an fMRI pipeline. In this study, we shown visual stimuli belonging to 2 categories (faces, vehicles) and 4 sub-categories (cars, bikes, male and female faces) to 24 subjects. We collected, preprocessed and analysed the fMRI data, and performed a Multi Voxel Pattern Analysis on the beta images. The results from this analysis showed above chance accuracy in some ROIs for all the possible comparisons, except for perceptually similar categories (e.g., female vs male faces). But how do we actually know what categories are perceptually similar? One solution that I implement here is to feed our images to a computational model performing basic low-level perceptual tasks, and run a similar classification analysis on the activation of the network. Here we chose the first convolutional layer of AlexNet. If we see above chance classification accuracy between two categorie of stimuli, then we can assume that these two categories are perceptually different for the network. Answering this question can help us understanding the nature (categorical vs. perceptual) of the information we found in our fMRI data. 

This is what the script does in a nutshell: we import the images, we load the netwok and plot the weights of the first convolutiona layer. we pass all the images through the network, and we extract the activation from the first convolutional layer for each image. We apply different levels of noise to the dataset, reduce the dimensionality and perform a classification analysis. Finally, we plot the results.

## fMRI_MVPA_from_SPM_betas
How can we know what information is coded in a specific Region of interest? Does the foveal cortex process information about peripheral stimuli? And what is the nature of the information ( categorical? perceptual?)? I wrote this script to perform a ROI MVPA analysis on fMRI data to answer these questions. In this script we use precomputed retinotopic and functional Regions of Interest (ROIs) to mask beta images (see section below for some more info about the study) generated by SPM, and we perform a cross-validated (GroupKFold) classification analysis on each ROI and for all the comparisons (e.g., bike vs cars, etc.) independently. We can decide if we want to perform feature selection on the dataset, and if we wanto to find the optimal classifier parameters with a cross-validated grid search. To import the beta images, we interact with the SPM.mat file from Python in order to map the betas. We finally plot the results.

## visualize_retinotopy
In this script I showcase three main things: 1. an implementation of a surface-based retinotopic mapping method developed by Noah Benson. I used this method to generate retinotopic ROIs based on subjects' anatomy, that I then used to mask fMRI data for subsequent analysis. 2. an interesting function to pass nuisance regressors from fMRIprep to nilearn for denoising 3. extraction and plotting of timeseries from surface data (with and without denoising strategies)
