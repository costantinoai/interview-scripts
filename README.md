# interview-scripts

A few polished scripts for postdoc interviews. The code is in Python and uploaded in Jupyter Notebooks for easy visualization. You can download the script or open them in your browser. More info:

## AlexNet_embeddings
How can we know if perceptual (dis)similarity can reasonably explain a pattern of results? In other words: what categories of images are perceptually distinct from each other, and what can this tell us about the brain? I wrote this script to explore the idea that the pattern of results we saw in our fMRI decoding analysis could be replicated by a computational model mimicking the early layers of the ventral visual stream.
This script is part of an fMRI pipeline. In this study, we showed visual stimuli belonging to 2 categories (faces, vehicles) and 4 sub-categories (cars, bikes, male and female faces) to 24 subjects. We collected, preprocessed and analyzed the fMRI data, and performed a Multi Voxel Pattern Analysis on the beta images. The results from this analysis showed above-chance accuracy in some ROIs for all the possible comparisons, except for perceptually similar categories (e.g., female vs male faces). But how do we know what categories are perceptually similar? One solution that I implement here is to feed our images to a computational model performing basic low-level perceptual tasks and run a similar classification analysis on the network's activation. Here we chose the first convolutional layer of AlexNet. If we see above-chance classification accuracy between two categories of stimuli, we can assume that these categories are perceptually different for the network. Answering this question can help us understand the nature (categorical vs. perceptual) of the information we found in our fMRI data.
This is what the script does in a nutshell: we import the images, load the network and plot the weights of the first convolutional layer. We pass all the images through the model, extracting the activation from the first convolutional layer for each image. We apply different levels of noise to the dataset, reduce the dimensionality and perform a classification analysis. Finally, we plot the results.

## fMRI_MVPA_from_SPM_betas
How can we know what information is coded in a specific region of interest? Does the foveal cortex process information about peripheral stimuli? And what is the nature ( categorical vs perceptual) of the information? I wrote this script to perform an ROI MVPA analysis on fMRI data to answer these questions. This script uses precomputed retinotopic and functional Regions of Interest (ROIs) to mask beta images (see the section above for some more info about the study) generated by SPM. We perform a cross-validated (GroupKFold) classification analysis on each ROI and for all the comparisons (e.g., bike vs cars, etc.) independently. We can decide if we want to perform feature selection on the dataset and if we want to find the optimal classifier parameters with a cross-validated grid search. To import the beta images, we interact with the SPM.mat file from Python to map the betas. We finally plot the results.

## visualize_retinotopy
This script showcases three main things: 1. an implementation of a surface-based retinotopic mapping method developed by Noah Benson. I used this method to generate retinotopic ROIs based on subjects' anatomy that I then used to mask fMRI data for subsequent analysis. 2. an interesting function to pass nuisance regressors from fMRIprep to nilearn for denoising 3. extraction and plotting of time series from surface data (with and without denoising strategies)
